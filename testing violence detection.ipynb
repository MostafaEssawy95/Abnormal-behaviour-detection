{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06d60434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30b675bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#violent_videos_folder=r'C:\\myfiles\\violence detection pytorch\\final_violent_videos'\n",
    "#all_violent_videos=os.listdir(violent_videos_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6547cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "videos_folder=r'C:\\myfiles\\manual dataset\\non violent videos'\n",
    "violent_videos=os.listdir(videos_folder)\n",
    "count=1\n",
    "for i in range(len(violent_videos)):\n",
    "    \n",
    "    cap=cv2.VideoCapture(os.path.join(videos_folder,violent_videos[i]))\n",
    "    while (cap.isOpened()):\n",
    "        success,frame=cap.read()\n",
    "        if success ==False:\n",
    "                break\n",
    "        cv2.imwrite(r'C:\\myfiles\\final dataset\\non violent frames\\\\'+'non violent '+str(count)+'.jpg',frame)\n",
    "        count+=1\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbad4f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_snippets import *\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os  \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_snippets import *\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b45e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class violence_dataset(Dataset):\n",
    "    def __init__(self,folder):\n",
    "        #if folder =='training':\n",
    "        #    self.frames=r'C:\\myfiles\\new_violence_detection\\all_training'\n",
    "        #    self.items=os.listdir(r'C:\\myfiles\\new_violence_detection\\all_training')\n",
    "    \n",
    "        if folder=='testing':\n",
    "            self.frames=r'C:\\myfiles\\final dataset\\all_testing'\n",
    "            self.items=os.listdir(r'C:\\myfiles\\final dataset\\all_testing')\n",
    "            \n",
    "    def __getitem__(self,ix):\n",
    "        temp=ix\n",
    "        if self.frames==r'C:\\myfiles\\final dataset\\all_testing':\n",
    "            total_images_list=[]\n",
    "            count=0\n",
    "            local_images=[]\n",
    "            image1=cv2.imread(os.path.join(self.frames,os.listdir(self.frames)[ix+9*ix]))\n",
    "            image1=cv2.resize(image1,(100,100))\n",
    "            image1=torch.Tensor(image1).permute(2,0,1)\n",
    "            #image1=torch.unsqueeze(image1,0)\n",
    "            #local_images.append(image)\n",
    "            image2=cv2.imread(os.path.join(self.frames,os.listdir(self.frames)[ix+1+9*ix]))\n",
    "            image2=cv2.resize(image2,(100,100))\n",
    "            image2=torch.Tensor(image2).permute(2,0,1)\n",
    "            #image2=torch.unsqueeze(image2,0)\n",
    "            #local_images.append(image)\n",
    "            image3=cv2.imread(os.path.join(self.frames,os.listdir(self.frames)[ix+2+9*ix]))\n",
    "            image3=cv2.resize(image3,(100,100))\n",
    "            image3=torch.Tensor(image3).permute(2,0,1)\n",
    "            #image3=torch.unsqueeze(image3,0)\n",
    "            #local_images.append(image)\n",
    "            image4=cv2.imread(os.path.join(self.frames,os.listdir(self.frames)[ix+3+9*ix]))\n",
    "            image4=cv2.resize(image4,(100,100))\n",
    "            image4=torch.Tensor(image4).permute(2,0,1)\n",
    "            #image4=torch.unsqueeze(image4,0)\n",
    "            #local_images.append(image)\n",
    "            image5=cv2.imread(os.path.join(self.frames,os.listdir(self.frames)[ix+4+9*ix]))\n",
    "            image5=cv2.resize(image5,(100,100))\n",
    "            image5=torch.Tensor(image5).permute(2,0,1)\n",
    "            #image5=torch.unsqueeze(image5,0)\n",
    "            #local_images.append(image)\n",
    "            image6=cv2.imread(os.path.join(self.frames,os.listdir(self.frames)[ix+5+9*ix]))\n",
    "            image6=cv2.resize(image6,(100,100))\n",
    "            image6=torch.Tensor(image6).permute(2,0,1)\n",
    "            #image6=torch.unsqueeze(image6,0)\n",
    "            #local_images.append(image)\n",
    "            image7=cv2.imread(os.path.join(self.frames,os.listdir(self.frames)[ix+6+9*ix]))\n",
    "            image7=cv2.resize(image7,(100,100))\n",
    "            image7=torch.Tensor(image7).permute(2,0,1)\n",
    "            #image7=torch.unsqueeze(image7,0)\n",
    "            #local_images.append(image)\n",
    "            image8=cv2.imread(os.path.join(self.frames,os.listdir(self.frames)[ix+7+9*ix]))\n",
    "            image8=cv2.resize(image8,(100,100))\n",
    "            image8=torch.Tensor(image8).permute(2,0,1)\n",
    "            #image8=torch.unsqueeze(image8,0)\n",
    "            #local_images.append(image)\n",
    "            image9=cv2.imread(os.path.join(self.frames,os.listdir(self.frames)[ix+8+9*ix]))\n",
    "            image9=cv2.resize(image9,(100,100))\n",
    "            image9=torch.Tensor(image9).permute(2,0,1)\n",
    "            #image9=torch.unsqueeze(image9,0)\n",
    "            #local_images.append(image)\n",
    "            image10=cv2.imread(os.path.join(self.frames,os.listdir(self.frames)[ix+9+9*ix]))\n",
    "            image10=cv2.resize(image10,(100,100))\n",
    "            image10=torch.Tensor(image10).permute(2,0,1)\n",
    "            #image10=torch.unsqueeze(image10,0)\n",
    "            #image=torch.cat((image1,image2,image3,image4,image5,image6,image7,image8,image9,image10),0)\n",
    "            image_path=os.path.join(self.frames,os.listdir(self.frames)[ix+9*ix])\n",
    "            if image_path.split('\\\\')[-1][0]=='n':\n",
    "                    label=torch.Tensor([0]).long()\n",
    "            elif  image_path.split('\\\\')[-1][0]=='v':\n",
    "                    label=torch.Tensor([1]).long() \n",
    "            #total_frame=torch.cat((image1,image2,image3,image4,image5,image6,image7,image8,image9,image10),0 )       \n",
    "                    \n",
    "            return image1.to(device),image2.to(device),image3.to(device),image4.to(device),image5.to(device),image6.to(device),image7.to(device),image8.to(device),image9.to(device),image10.to(device),label.to(device)        \n",
    "            #return total_frame        \n",
    "    def __len__(self):\n",
    "        return int(len(self.items)/10)            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eded218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds=violence_dataset('testing')\n",
    "test_dl = DataLoader(test_ds, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69573b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16_bn\n",
    "class violent_net(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "            super().__init__()\n",
    "            self.encoder = vgg16_bn(pretrained=pretrained).features  \n",
    "            self.convblock=nn.Sequential(*self.encoder[:])\n",
    "            self.flatten=nn.Flatten()\n",
    "            self.lstm=nn.LSTM(input_size=4608, hidden_size=20, num_layers=2,bidirectional=True)\n",
    "            self.flatten=nn.Flatten()\n",
    "            self.linear1=nn.Linear(400,32)\n",
    "            self.activation1=nn.ReLU()\n",
    "            self.linear2=nn.Linear(32,16)\n",
    "            self.activation2=nn.ReLU()\n",
    "            self.linear3=nn.Linear(16,2)\n",
    "            self.activation3=nn.Softmax(dim=1)\n",
    "            \n",
    "    def forward(self, x1,x2,x3,x4,x5,x6,x7,x8,x9,x10):\n",
    "        x1 = self.convblock(x1)\n",
    "        x1=self.flatten(x1)\n",
    "        x2 = self.convblock(x2)\n",
    "        x2=self.flatten(x2)\n",
    "        x3 = self.convblock(x3)\n",
    "        x3=self.flatten(x3)\n",
    "        x4 = self.convblock(x4)\n",
    "        x4=self.flatten(x4)\n",
    "        x5 = self.convblock(x5)\n",
    "        x5=self.flatten(x5)\n",
    "        x6 = self.convblock(x6)\n",
    "        x6=self.flatten(x6)\n",
    "        x7 = self.convblock(x7)\n",
    "        x7=self.flatten(x7)\n",
    "        x8 = self.convblock(x8)\n",
    "        x8=self.flatten(x8)\n",
    "        x9 = self.convblock(x9)\n",
    "        x9=self.flatten(x9)\n",
    "        x10 = self.convblock(x10)\n",
    "        x10=self.flatten(x10)\n",
    "        x=torch.cat((x1,x2,x3,x4,x5,x6,x7,x8,x9,x10),0)\n",
    "        x=torch.unsqueeze(x,1)\n",
    "        x=self.lstm(x)\n",
    "        x=torch.cat((x[0][0],x[0][1],x[0][2],x[0][3],x[0][4],x[0][5],x[0][6],x[0][7],x[0][8],x[0][9]),1)\n",
    "        x=self.flatten(x)\n",
    "        x=self.linear1(x)\n",
    "        x=self.activation1(x)\n",
    "        x=self.linear2(x)\n",
    "        x=self.activation2(x)\n",
    "        x=self.linear3(x)\n",
    "        #x=self.activation3(x)\n",
    "        \n",
    "        \n",
    "       # return x1,x2,x3,x4,x5,x6,x7,x8,x9,x10\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "874f2842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = violent_net()\n",
    "model.load_state_dict(torch.load(r'C:\\myfiles\\lastgpmodel.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4da368bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "545e9b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# iterate over test data\n",
    "for input1,input2,input3,input4,input5,input6,input7,input8,input9,input10, labels in test_dl:\n",
    "        output = model(input1,input2,input3,input4,input5,input6,input7,input8,input9,input10) # Feed Network\n",
    "\n",
    "        output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
    "        y_pred.extend(output) # Save Prediction\n",
    "        \n",
    "        labels = labels.data.cpu().numpy()\n",
    "        y_true.extend(labels) # Save Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b021933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 84,  40],\n",
       "       [ 23, 102]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = ('not violent', 'violent')\n",
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "cf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36e103c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_cm = pd.DataFrame(cf_matrix, index = [i for i in classes],\n",
    "                    columns = [i for i in classes])\n",
    "plt.figure(figsize = (12,7))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5351d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " non violent       0.79      0.68      0.73       124\n",
      "     violent       0.72      0.82      0.76       125\n",
      "\n",
      "    accuracy                           0.75       249\n",
      "   macro avg       0.75      0.75      0.75       249\n",
      "weighted avg       0.75      0.75      0.75       249\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['non violent', 'violent']    \n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c40d69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
